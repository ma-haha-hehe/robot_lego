import transformers
print(f"Transformers path: {transformers.__file__}")
print(f"Transformers version: {transformers.__version__}")
import cv2
import torch
import numpy as np
import time
from dataclasses import dataclass
from typing import List, Dict, Optional, Tuple, Union
from PIL import Image
from transformers import AutoModelForMaskGeneration, AutoProcessor, pipeline

# ================= æ•°æ®ç»“æ„å®šä¹‰ (Data Structures) =================

@dataclass
class BoundingBox:
    xmin: int
    ymin: int
    xmax: int
    ymax: int

    @property
    def xyxy(self) -> List[float]:
        return [self.xmin, self.ymin, self.xmax, self.ymax]

@dataclass
class DetectionResult:
    score: float
    label: str
    box: BoundingBox
    mask: Optional[np.array] = None

    @classmethod
    def from_dict(cls, detection_dict: Dict) -> 'DetectionResult':
        return cls(score=detection_dict['score'],
                   label=detection_dict['label'],
                   box=BoundingBox(xmin=detection_dict['box']['xmin'],
                                   ymin=detection_dict['box']['ymin'],
                                   xmax=detection_dict['box']['xmax'],
                                   ymax=detection_dict['box']['ymax']))

# ================= è¾…åŠ©å‡½æ•° (Helper Functions) =================
# ================= NMS (å»é‡) è¾…åŠ©å‡½æ•° =================
# ==========================================
# ğŸ“ æ–°å¢åŠŸèƒ½ï¼šæ ¹æ®å½¢çŠ¶è‡ªåŠ¨åŒºåˆ†é•¿/çŸ­ç§¯æœ¨
# ==========================================

def identify_brick_type(detection: DetectionResult) -> str:
    """
    é€šè¿‡è®¡ç®—è¾¹ç•Œæ¡†çš„é•¿å®½æ¯”ï¼Œåˆ¤æ–­æ˜¯é•¿ç§¯æœ¨è¿˜æ˜¯çŸ­ç§¯æœ¨ã€‚
    """
    xmin, ymin, xmax, ymax = detection.box.xyxy
    
    width = xmax - xmin
    height = ymax - ymin
    
    # é¿å…é™¤ä»¥é›¶
    if height == 0 or width == 0:
        return "Unknown"

    # è®¡ç®—é•¿å®½æ¯” (é•¿è¾¹ / çŸ­è¾¹)
    # ç»“æœæ€»æ˜¯ >= 1.0
    # å¦‚æœæ˜¯æ­£æ–¹å½¢ï¼Œç»“æœæ¥è¿‘ 1.0
    # å¦‚æœæ˜¯é•¿æ–¹å½¢ï¼Œç»“æœä¼šæ˜¾è‘—å¤§äº 1.0
    ratio = max(width, height) / min(width, height)
    
    # --- é˜ˆå€¼åˆ¤æ–­é€»è¾‘ ---
    # 2x2 ç§¯æœ¨é€šå¸¸æ¥è¿‘ 1.0ï¼Œç¨å¾®æœ‰ç‚¹è¯¯å·®å¯èƒ½åˆ° 1.3
    # 2x4 ç§¯æœ¨é€šå¸¸åœ¨ 1.5 åˆ° 2.0 ä¹‹é—´
    
    if ratio < 1.4:
        return "Short (2x2)"  # æ­£æ–¹å½¢
    else:
        return "Long (2x4)"   # é•¿æ–¹å½¢

# ==========================================

def calculate_iou(box1, box2):
    """è®¡ç®—ä¸¤ä¸ªæ¡†çš„é‡å åº¦ (Intersection over Union)"""
    # è®¡ç®—é‡å åŒºåŸŸçš„åæ ‡
    x1 = max(box1.xmin, box2.xmin)
    y1 = max(box1.ymin, box2.ymin)
    x2 = min(box1.xmax, box2.xmax)
    y2 = min(box1.ymax, box2.ymax)

    # è®¡ç®—é‡å é¢ç§¯
    intersection = max(0, x2 - x1) * max(0, y2 - y1)

    # è®¡ç®—ä¸¤ä¸ªæ¡†å„è‡ªçš„é¢ç§¯
    box1_area = (box1.xmax - box1.xmin) * (box1.ymax - box1.ymin)
    box2_area = (box2.xmax - box2.xmin) * (box2.ymax - box2.ymin)

    # è®¡ç®—å¹¶é›†é¢ç§¯
    union = box1_area + box2_area - intersection

    return intersection / union if union > 0 else 0

def filter_double_detections(detections: List[DetectionResult], iou_threshold: float = 0.5) -> List[DetectionResult]:
    """
    NMS ç®—æ³•ï¼šå¦‚æœä¸¤ä¸ªæ¡†é‡å è¶…è¿‡ 50%ï¼Œåªä¿ç•™åˆ†æ•°é«˜çš„é‚£ä¸ªã€‚
    è¿™æ ·åŒä¸€ä¸ªç‰©ä½“å°±ä¸ä¼šæœ‰ä¸¤ä¸ªæ ‡ç­¾äº†ã€‚
    """
    if not detections:
        return []
    
    # 1. æŒ‰åˆ†æ•°ä»é«˜åˆ°ä½æ’åº (åˆ†æ•°é«˜çš„æ’å‰é¢)
    detections = sorted(detections, key=lambda x: x.score, reverse=True)
    
    keep = []
    
    for current in detections:
        is_duplicate = False
        # æ£€æŸ¥å½“å‰æ¡†æ˜¯å¦å’Œæˆ‘ä»¬å·²ç»ä¿ç•™çš„æ¡†é‡å 
        for kept in keep:
            if calculate_iou(current.box, kept.box) > iou_threshold:
                is_duplicate = True # æ‰¾åˆ°äº†é‡å ä¸”åˆ†æ•°æ›´é«˜çš„å¤§å“¥ï¼Œå½“å‰è¿™ä¸ªå°å¼Ÿå°±ä¸è¦äº†
                break
        
        if not is_duplicate:
            keep.append(current)
            
    return keep

def get_boxes(results: List[DetectionResult]) -> List[List[List[float]]]:
    boxes = []
    for result in results:
        xyxy = result.box.xyxy
        boxes.append(xyxy)
    return [boxes]

def mask_to_polygon(mask: np.ndarray) -> List[List[int]]:
    # å¯»æ‰¾è½®å»“
    contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if not contours:
        return []
    # æ‰¾åˆ°æœ€å¤§çš„è½®å»“
    largest_contour = max(contours, key=cv2.contourArea)
    polygon = largest_contour.reshape(-1, 2).tolist()
    return polygon

def polygon_to_mask(polygon: List[Tuple[int, int]], image_shape: Tuple[int, int]) -> np.ndarray:
    mask = np.zeros(image_shape, dtype=np.uint8)
    pts = np.array(polygon, dtype=np.int32)
    cv2.fillPoly(mask, [pts], color=(1,))
    return mask

def refine_masks(masks: torch.BoolTensor, polygon_refinement: bool = False) -> List[np.ndarray]:
    masks = masks.cpu().float()
    masks = masks.permute(0, 2, 3, 1)
    masks = masks.mean(axis=-1)
    masks = (masks > 0).int()
    masks = masks.numpy().astype(np.uint8)
    masks = list(masks)

    if polygon_refinement:
        for idx, mask in enumerate(masks):
            shape = mask.shape
            polygon = mask_to_polygon(mask)
            if polygon:
                mask = polygon_to_mask(polygon, shape)
                masks[idx] = mask
    return masks

def annotate_video_frame(frame_bgr: np.ndarray, detection_results: List[DetectionResult]) -> np.ndarray:
    """
    åœ¨ OpenCV çš„ BGR å›¾åƒä¸Šç›´æ¥ç»˜åˆ¶æ¡†å’Œæ©ç 
    """
    # å¤åˆ¶ä¸€ä»½ä»¥å…ä¿®æ”¹åŸå›¾
    image_cv2 = frame_bgr.copy()

    for detection in detection_results:
        label = detection.label
        score = detection.score
        box = detection.box
        mask = detection.mask

        # éšæœºé¢œè‰²
        color = np.random.randint(0, 256, size=3).tolist()

        # 1. ç”»æ¡†
        cv2.rectangle(image_cv2, (box.xmin, box.ymin), (box.xmax, box.ymax), color, 2)
        
        # 2. å†™æ ‡ç­¾
        text = f'{label}: {score:.2f}'
        cv2.putText(image_cv2, text, (box.xmin, box.ymin - 10), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

        # 3. ç”»æ©ç  (Mask)
        if mask is not None:
            # åˆ›å»ºåŠé€æ˜é®ç½©
            mask_uint8 = (mask * 255).astype(np.uint8)
            
            # æ–¹æ³• A: ä»…ç”»è½®å»“ (é€Ÿåº¦å¿«ï¼Œæ¸…æ™°)
            contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            cv2.drawContours(image_cv2, contours, -1, color, 2)
            
            # æ–¹æ³• B: é¢œè‰²å¡«å…… (å¯é€‰ï¼Œè¿™é‡Œç”¨å åŠ æ–¹å¼)
            colored_mask = np.zeros_like(image_cv2, dtype=np.uint8)
            colored_mask[mask > 0] = color
            # å°†æ©ç ä¸åŸå›¾æ··åˆ
            image_cv2 = cv2.addWeighted(image_cv2, 1.0, colored_mask, 0.4, 0)

    return image_cv2

# ================= æ ¸å¿ƒé€»è¾‘ (Core Logic) =================

def detect_video(
    image: Image.Image,
    detector,
    labels: List[str],
    threshold: float = 0.3
) -> List[DetectionResult]:
    # å¤„ç†æ ‡ç­¾æ ¼å¼ (åŠ ä¸Š . ä»¥æé«˜ DINO å‡†ç¡®ç‡)
    labels = [label if label.endswith(".") else label + "." for label in labels]
    
    results = detector(image, candidate_labels=labels, threshold=threshold)
    return [DetectionResult.from_dict(result) for result in results]

def segment_video(
        image: Image.Image,
        detection_results: List[DetectionResult],
        segmentator,
        processor,
        polygon_refinement: bool = False,
        device: str = "cpu"
) -> List[DetectionResult]:
    
    boxes = get_boxes(detection_results)
    
    # é¢„å¤„ç†è¾“å…¥
    inputs = processor(images=image, input_boxes=boxes, return_tensors="pt")

    # [å…³é”®ä¿®å¤] é’ˆå¯¹ MPS (Mac) è®¾å¤‡çš„ç±»å‹é”™è¯¯ä¿®å¤
    if 'input_boxes' in inputs and inputs['input_boxes'].dtype == torch.float64:
        inputs['input_boxes'] = inputs['input_boxes'].to(torch.float32)

    # ç§»åŠ¨åˆ°è®¾å¤‡ (GPU/CPU)
    inputs = inputs.to(device)

    # æ¨ç†
    with torch.no_grad():
        outputs = segmentator(**inputs)

    # åå¤„ç†æ©ç 
    masks = processor.post_process_masks(
        masks=outputs.pred_masks,
        original_sizes=inputs.original_sizes,
        reshaped_input_sizes=inputs.reshaped_input_sizes
    )[0]

    masks = refine_masks(masks, polygon_refinement)

    # å°†æ©ç åˆ†é…å›ç»“æœå¯¹è±¡
    for detection_result, mask in zip(detection_results, masks):
        detection_result.mask = mask

    return detection_results

# ================= ä¸»ç¨‹åº (Main) =================

if __name__ == "__main__":
    
    # --- 1. é…ç½®å‚æ•° (Settings) ---
    WEBCAM_ID = 4              # ğŸš¨ å¦‚æœæ‰“ä¸å¼€ï¼Œå°è¯•æ”¹æˆ 0, 2, 4, 6
    PROCESS_SIZE = (640, 480)  # é™ä½åˆ†è¾¨ç‡ä»¥æé«˜ FPS
    CONF_THRESHOLD = 0.35      # ç¨å¾®è°ƒé«˜é—¨æ§›ï¼Œå‡å°‘è¯¯è¯†åˆ«
    
    # æç¤ºè¯ï¼šå»ºè®®ç”¨æè¿°æ€§çš„è¯
    LABELS = ["black brick.", "white brick.","blue brick.","red brick."] 

    DETECTOR_ID = "IDEA-Research/grounding-dino-tiny"
    SEGMENTER_ID = "facebook/sam-vit-base"

    # --- 2. è®¾å¤‡æ£€æµ‹ ---
    if torch.cuda.is_available():
        DEVICE = "cuda"
        print(">>> æ­£åœ¨ä½¿ç”¨ NVIDIA GPU (CUDA) ğŸš€")
    elif torch.backends.mps.is_available():
        DEVICE = "mps"

        print(">>> æ­£åœ¨ä½¿ç”¨ Apple Silicon (MPS)")
    else:
        DEVICE = "cpu"
        print(">>> è­¦å‘Šï¼šæ­£åœ¨ä½¿ç”¨ CPUï¼Œé€Ÿåº¦è¾ƒæ…¢")

    # --- 3. åŠ è½½æ¨¡å‹ ---
    print(">>> æ­£åœ¨åŠ è½½æ¨¡å‹ (è¯·ç¨å€™)...")
    try:
        detector_pipeline = pipeline(model=DETECTOR_ID, task="zero-shot-object-detection", device=DEVICE)
        sam_model = AutoModelForMaskGeneration.from_pretrained(SEGMENTER_ID).to(DEVICE)
        sam_processor = AutoProcessor.from_pretrained(SEGMENTER_ID)
        print(">>> âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        exit()

    # --- 4. æ‰“å¼€æ‘„åƒå¤´ ---
    cap = cv2.VideoCapture(WEBCAM_ID)
    if not cap.isOpened():
        print(f"âŒ é”™è¯¯ï¼šæ— æ³•æ‰“å¼€æ‘„åƒå¤´ ID {WEBCAM_ID}")
        print("   å»ºè®®: è¿è¡Œ 'ls /dev/video*' æŸ¥çœ‹å¯ç”¨è®¾å¤‡ï¼Œæˆ–å°è¯•æ›´æ”¹ WEBCAM_ID")
        exit()

    print(">>> ç³»ç»Ÿè¿è¡Œä¸­... æŒ‰ 'q' é”®é€€å‡º")

    while cap.isOpened():
        start_time = time.time()
        ret, frame = cap.read()
        if not ret: break

        # A. é¢„å¤„ç†
        frame = cv2.flip(frame, 1)
        frame_resized = cv2.resize(frame, PROCESS_SIZE)
        image_pil = Image.fromarray(cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB))

        # B. ç›®æ ‡æ£€æµ‹ (Grounding DINO)
        detections = detect_video(image_pil, detector_pipeline, LABELS, CONF_THRESHOLD)

        # ğŸ”¥ C. å…³é”®æ­¥éª¤ï¼šNMS å»é‡ (åªä¿ç•™é‡å ç‰©ä½“ä¸­åˆ†æ•°æœ€é«˜çš„ä¸€ä¸ª)
        if detections:
            detections = filter_double_detections(detections, iou_threshold=0.5)

        # D. å®ä¾‹åˆ†å‰² (SAM)
        if detections:
            detections = segment_video(image_pil, detections, sam_model, sam_processor, device=DEVICE)
            annotated_frame = annotate_video_frame(frame_resized, detections)
        else:
            annotated_frame = frame_resized

        # E. æ˜¾ç¤º FPS
        fps = 1.0 / (time.time() - start_time)
        cv2.putText(annotated_frame, f"FPS: {fps:.1f}", (10, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)

        # F. æ˜¾ç¤ºç»“æœ
        cv2.imshow("Lego Sorter AI", annotated_frame)
        
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()