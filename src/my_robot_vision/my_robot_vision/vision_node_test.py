#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import cv2
import torch
import numpy as np
import pyrealsense2 as rs
import os
import yaml
import trimesh
import time
import sys
from PIL import Image
from dataclasses import dataclass
from typing import Optional
from transformers import pipeline
from scipy.spatial.transform import Rotation as R

# ================= 1. è·¯å¾„ä¸ç¯å¢ƒé…ç½® =================
FP_REPO = "/FoundationPose"
RESULT_FILE = "/shared_data/active_task.yaml"  # æ¡æ‰‹ä¿¡å·æ–‡ä»¶
TASKS_YAML = "/vision_code/tasks.yaml"        # ä»»åŠ¡åˆ—è¡¨
CAMERA_PARAMS_YAML = "/vision_code/camera_params.yaml" # ç›¸æœºå¤–å‚çŸ©é˜µ
MESH_DIR = "/FoundationPose/meshes"           # STLæ¨¡å‹å­˜æ”¾å¤„
ASSEMBLY_CENTER_BASE = np.array([0.25,0, 0.0]) # ç»„è£…åŒºåŸºå‡†ä¸­å¿ƒ

if FP_REPO not in sys.path:
    sys.path.append(FP_REPO)
    sys.path.append(os.path.join(FP_REPO, "root"))

from estimater import FoundationPose, ScorePredictor, PoseRefinePredictor

# å°è¯•åŠ è½½ SAM ç»„ä»¶
try:
    from segment_anything import sam_model_registry, SamPredictor
    SAM_AVAILABLE = True
except ImportError:
    SAM_AVAILABLE = False

@dataclass
class BoundingBox:
    xmin: int; ymin: int; xmax: int; ymax: int
    @property
    def xyxy(self): return [self.xmin, self.ymin, self.xmax, self.ymax]

@dataclass
class DetectionResult:
    score: float; label: str; box: BoundingBox

# ================= 2. æ ¸å¿ƒä¼°è®¡ç±» =================
class LegoPoseEstimator:
    def __init__(self, scorer, refiner):
        self.scorer = scorer
        self.refiner = refiner

    def update_mesh(self, mesh_path):
        """ åŠ è½½å¹¶æ ¡å‡† 3D æ¨¡å‹ï¼Œç¡®ä¿åæ ‡åŸç‚¹åœ¨å‡ ä½•ä¸­å¿ƒ """
        self.mesh = trimesh.load(mesh_path)
        if np.linalg.norm(self.mesh.extents) > 0.1: 
            self.mesh.apply_scale(0.001)
        
        # å±…ä¸­å¤„ç†ï¼Œæ¶ˆé™¤æ¨¡å‹è‡ªå¸¦çš„åç§»
        self.mesh.vertices -= self.mesh.bounds.mean(axis=0)
        model_pts, _ = trimesh.sample.sample_surface(self.mesh, 2048)
        self.model_pts = torch.from_numpy(model_pts.astype(np.float32)).cuda()
        
        self.estimator = FoundationPose(
            model_pts=self.model_pts, model_normals=None, mesh=self.mesh,
            scorer=self.scorer, refiner=self.refiner
        )

# ================= 3. è‡ªåŠ¨åŒ–è§†è§‰èŠ‚ç‚¹ç±» =================
class RobotVisionNode:
    def __init__(self):
        # åŠ è½½ç›¸æœºå¤–å‚ (Camera to Base)
        with open(CAMERA_PARAMS_YAML, 'r') as f:
            params = yaml.safe_load(f)
        self.T_base_camera = np.array(params['extrinsic_matrix']).reshape(4, 4)
        
        with open(TASKS_YAML, 'r') as f:
            self.task_list = yaml.safe_load(f)['tasks']

        self.init_realsense()
        
        # åˆå§‹åŒ– GroundingDINO
        self.detector = pipeline(model="IDEA-Research/grounding-dino-tiny", task="zero-shot-object-detection", device="cuda")
        
        # åˆå§‹åŒ– SAM (Segment Anything)
        if SAM_AVAILABLE:
            sam = sam_model_registry["vit_h"](checkpoint="/FoundationPose/weights/sam_vit_h_4b8939.pth").to("cuda")
            self.sam_predictor = SamPredictor(sam)

        # åˆå§‹åŒ– FoundationPose
        self.pose_est = LegoPoseEstimator(ScorePredictor(), PoseRefinePredictor())

    def init_realsense(self):
        self.pipeline = rs.pipeline()
        config = rs.config()
        config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)
        config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)
        profile = self.pipeline.start(config)
        intr = profile.get_stream(rs.stream.color).as_video_stream_profile().get_intrinsics()
        self.K_MATRIX = np.array([[intr.fx, 0, intr.ppx], [0, intr.fy, intr.ppy], [0, 0, 1]])
        self.align = rs.align(rs.stream.color)

    def run(self):
        for task in self.task_list:
            name = task['name']
            print(f"\n" + "="*60)
            print(f"ğŸ¯ å½“å‰è¯†åˆ«ç›®æ ‡: {name}")
            
            mesh_path = self.get_mesh_path(name)
            self.pose_est.update_mesh(mesh_path)

            # --- é˜¶æ®µ 1: GroundingDINO å¯è§†åŒ– ---
            print(f"ğŸ‘€ é˜¶æ®µ 1: æ­£åœ¨è§‚å¯Ÿåœºæ™¯ (8.0s)...")
            best_det = None
            max_score = -1
            best_img_bgr = None
            obs_start = time.time()
            
            while (time.time() - obs_start) < 8.0:
                frames = self.pipeline.wait_for_frames()
                img = np.asanyarray(self.align.process(frames).get_color_frame().get_data())
                img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
                
                results = self.detector(img_pil, candidate_labels=[name, "lego block."], threshold=0.3)
                
                # åˆ›å»ºæ£€æµ‹å¯è§†åŒ–ç”»å¸ƒ
                dino_vis = img.copy()
                for r in results:
                    if r['score'] > max_score:
                        max_score = r['score']
                        best_det = DetectionResult(r['score'], r['label'], BoundingBox(**r['box']))
                        best_img_bgr = img.copy()
                    
                    # ç»˜åˆ¶æ‰€æœ‰å€™é€‰æ¡†
                    b = r['box']
                    cv2.rectangle(dino_vis, (b['xmin'], b['ymin']), (b['xmax'], b['ymax']), (0, 255, 0), 2)
                    cv2.putText(dino_vis, f"{r['label']}: {r['score']:.2f}", (b['xmin'], b['ymin']-10), 
                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
                
                cv2.putText(dino_vis, f"Scanning... Time Left: {8-(time.time()-obs_start):.1f}s", (20, 35), 0, 0.7, (0, 255, 255), 2)
                cv2.imshow("Step 1: GroundingDINO Detection", dino_vis)
                cv2.waitKey(1)

            if not best_det:
                print("âŒ æ— æ³•è¯†åˆ«ç‰©ä½“ï¼Œè·³è¿‡å½“å‰ä»»åŠ¡ã€‚"); continue

            # --- é˜¶æ®µ 2: SAM åƒç´ çº§åˆ†å‰²å¯è§†åŒ– ---
            print(f"âœ‚ï¸ é˜¶æ®µ 2: SAM ç²¾ç¡®æ‰£å›¾...")
            if SAM_AVAILABLE:
                self.sam_predictor.set_image(cv2.cvtColor(best_img_bgr, cv2.COLOR_BGR2RGB))
                masks, _, _ = self.sam_predictor.predict(box=np.array(best_det.box.xyxy), multimask_output=False)
                refined_mask = masks[0]
            else:
                refined_mask = np.zeros(best_img_bgr.shape[:2], dtype=bool)
                b = best_det.box
                refined_mask[b.ymin:b.ymax, b.xmin:b.xmax] = True

            # åˆ›å»º SAM é®ç½©å¯è§†åŒ–
            sam_vis = best_img_bgr.copy()
            # å°†é®ç½©åŒºåŸŸæŸ“è‰²ï¼ˆè“è‰²å åŠ å±‚ï¼‰
            sam_vis[refined_mask] = sam_vis[refined_mask] * 0.5 + np.array([255, 0, 0]) * 0.5
            cv2.imshow("Step 2: SAM Mask Visualization", sam_vis.astype(np.uint8))
            cv2.waitKey(500) # æš‚åœåŠç§’å±•ç¤ºåˆ†å‰²ç»“æœ

            # --- é˜¶æ®µ 3: FoundationPose ä½å§¿å¯è§†åŒ– ---
            print(f"ğŸ’ é˜¶æ®µ 3: ä½å§¿è§£ç®—ä¸ç²¾ç‚¼ (4.0s)...")
            pose_samples = []
            refine_start = time.time()
            
            while (time.time() - refine_start) < 4.0:
                frames = self.pipeline.wait_for_frames()
                aligned = self.align.process(frames)
                img = np.asanyarray(aligned.get_color_frame().get_data())
                depth_m = np.asanyarray(aligned.get_depth_frame().get_data()).astype(np.float32) / 1000.0
                
                T_curr = self.pose_est.estimator.register(
                    K=self.K_MATRIX, rgb=img, depth=depth_m, 
                    ob_mask=refined_mask, iteration=30
                )
                
                if T_curr is not None:
                    pose_samples.append(T_curr)
                    # visualize_result å†…éƒ¨å·²åŒ…å«ç»˜åˆ¶ Base åæ ‡å’Œåæ ‡è½´çš„é€»è¾‘
                    pose_vis = self.visualize_result(img.copy(), T_curr)
                    cv2.imshow("Step 3: FoundationPose Refinement", pose_vis)
                
                cv2.waitKey(1)

            if len(pose_samples) > 0:
                self.send_to_robot(name, pose_samples[-1], task)
            else:
                print("âŒ æœªæ•è·åˆ°æœ‰æ•ˆçš„ä½å§¿æ ·æœ¬ã€‚")

    def visualize_result(self, image, T_cam_obj):
        """ ç»¼åˆå¯è§†åŒ–ï¼šç»˜åˆ¶å§¿æ€è½´å’Œ Base ç©ºé—´åæ ‡ """
        # 1. ç»˜åˆ¶ 3D åæ ‡è½´
        length = 0.05
        axis_pts_3d = np.float32([[0,0,0], [length,0,0], [0,length,0], [0,0,length]])
        R_m, t_vec = T_cam_obj[:3, :3], T_cam_obj[:3, 3]
        pts_cam = (R_m @ axis_pts_3d.T).T + t_vec
        
        pts_2d = []
        for p in pts_cam:
            u = int(self.K_MATRIX[0,0] * p[0]/p[2] + self.K_MATRIX[0,2])
            v = int(self.K_MATRIX[1,1] * p[1]/p[2] + self.K_MATRIX[1,2])
            pts_2d.append((u, v))
        
        cv2.line(image, pts_2d[0], pts_2d[1], (0,0,255), 2) # X-Red
        cv2.line(image, pts_2d[0], pts_2d[2], (0,255,0), 2) # Y-Green
        cv2.line(image, pts_2d[0], pts_2d[3], (255,0,0), 2) # Z-Blue

        # 2. è®¡ç®—å¹¶ç»˜åˆ¶ç‰©ä½“çš„ Base åæ ‡
        # P_base = T_base_camera @ P_camera
        P_base = self.T_base_camera @ np.append(t_vec, 1.0)
        bx, by, bz = P_base[0], P_base[1], P_base[2]

        # åœ¨ä¸­å¿ƒä½ç½®ç”»ä¸ªåœ†ç‚¹
        cv2.circle(image, pts_2d[0], 5, (255, 255, 0), -1)
        
        # åœ¨åœ†ç‚¹æ—è¾¹æ ‡æ³¨ Base åæ ‡ä¿¡æ¯
        coord_txt = f"Base: [{bx:.3f}, {by:.3f}, {bz:.3f}]"
        cv2.putText(image, coord_txt, (pts_2d[0][0] + 10, pts_2d[0][1] - 10), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)
        return image

    def send_to_robot(self, name, T_cam_obj, task_cfg):
        """ åæ ‡æœ€ç»ˆå˜æ¢å¹¶å‘é€ä¿¡å·ç»™ C++ """
        T_base_pick = self.T_base_camera @ T_cam_obj

        # æå–æ¬§æ‹‰è§’ euler[0] yam
        r_pick = R.from_matrix(T_base_pick[:3, :3])
        euler_pick = r_pick.as_euler('zyx', degrees=False)
        detected_yaw = euler_pick[0]

        # 3. æ„é€ æ”¾ç½®å§¿æ€ï¼šRoll=pi, Pitch=0, Yaw=è¯†åˆ«åˆ°çš„å€¼
        # è¿™é‡Œçš„ pi (180åº¦) ç¡®ä¿å¤¹çˆªå‚ç›´å‘ä¸‹
        r_place = R.from_euler('xyz', [np.pi, 0, detected_yaw], degrees=False)
        pick_q = r_place.as_quat()
        
        # è®¡ç®—æ”¾ç½®ä½å§¿
        place_pos = ASSEMBLY_CENTER_BASE + np.array(task_cfg['place']['pos'])
        
        data = {
        'name': name,
        'pick': {
            'pos': T_base_pick[:3, 3].tolist(), 
            'orientation': r_pick.as_quat().tolist() # æŠ“å–æ—¶å®Œå…¨åŒ¹é…è¯†åˆ«å§¿æ€
        },
        'place': {
            'pos': place_pos.tolist(), 
            'orientation': place_q.tolist() # æ”¾ç½®æ—¶å‚ç›´å‘ä¸‹ä½†ä¿ç•™æ—‹è½¬
        }
        }
        
        with open(RESULT_FILE, 'w') as f:
            yaml.dump(data, f)
        print(f"âœ… å·²å†™å…¥ä¿¡å·æ–‡ä»¶ã€‚ç­‰å¾…æœºæ¢°è‡‚æ‰§è¡Œå¹¶æ¸…ç†...")
        
        while os.path.exists(RESULT_FILE):
            time.sleep(0.5)

    def get_mesh_path(self, task_name):
        keyword = "4x2" if ("2x4" in task_name or "4x2" in task_name) else "2x2"
        for f in os.listdir(MESH_DIR):
            if keyword in f and f.endswith(".stl"):
                return os.path.join(MESH_DIR, f)
        return ""

if __name__ == "__main__":
    node = RobotVisionNode()
    try:
        node.run()
    finally:
        cv2.destroyAllWindows()