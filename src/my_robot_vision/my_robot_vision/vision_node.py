#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import cv2
import torch
import numpy as np
import pyrealsense2 as rs
import os
import yaml
import trimesh
import time
import sys
from PIL import Image
from dataclasses import dataclass
from typing import Optional
from transformers import pipeline
from scipy.spatial.transform import Rotation as R

# ================= 1. è·¯å¾„ä¸ç¯å¢ƒé…ç½® =================
FP_REPO = "/FoundationPose"
RESULT_FILE = "/shared_data/active_task.yaml"  
TASKS_YAML = "/vision_code/tasks.yaml"        
CAMERA_PARAMS_YAML = "/vision_code/camera_params.yaml" 
MESH_DIR = "/FoundationPose/meshes"           
# ç»„è£…åŒºåŸºå‡†ï¼šç”¨äº Place åæ ‡è®¡ç®—
ASSEMBLY_CENTER_BASE = np.array([0.25, 0, 0.0]) 

if FP_REPO not in sys.path:
    sys.path.append(FP_REPO)
    sys.path.append(os.path.join(FP_REPO, "root"))

from estimater import FoundationPose, ScorePredictor, PoseRefinePredictor

try:
    from segment_anything import sam_model_registry, SamPredictor
    SAM_AVAILABLE = True
except ImportError:
    SAM_AVAILABLE = False

@dataclass
class BoundingBox:
    xmin: int; ymin: int; xmax: int; ymax: int
    @property
    def xyxy(self): return [self.xmin, self.ymin, self.xmax, self.ymax]

@dataclass
class DetectionResult:
    score: float; label: str; box: BoundingBox

# ================= 2. æ ¸å¿ƒä¼°è®¡ç±» =================
class LegoPoseEstimator:
    def __init__(self, scorer, refiner):
        self.scorer = scorer
        self.refiner = refiner

    def update_mesh(self, mesh_path):
        self.mesh = trimesh.load(mesh_path)
        if np.linalg.norm(self.mesh.extents) > 0.1: 
            self.mesh.apply_scale(0.001)
        self.mesh.vertices -= self.mesh.bounds.mean(axis=0)
        model_pts, _ = trimesh.sample.sample_surface(self.mesh, 2048)
        self.model_pts = torch.from_numpy(model_pts.astype(np.float32)).cuda()
        self.estimator = FoundationPose(
            model_pts=self.model_pts, model_normals=None, mesh=self.mesh,
            scorer=self.scorer, refiner=self.refiner
        )

# ================= 3. è‡ªåŠ¨åŒ–è§†è§‰èŠ‚ç‚¹ç±» =================
class RobotVisionNode:
    def __init__(self):
        with open(CAMERA_PARAMS_YAML, 'r') as f:
            params = yaml.safe_load(f)
        self.T_base_camera = np.array(params['extrinsic_matrix']).reshape(4, 4)
        
        with open(TASKS_YAML, 'r') as f:
            # è¯»å–ä»»åŠ¡åˆ—è¡¨ï¼Œæ³¨æ„é”®åéœ€ä¸ä½ çš„ plan ç”Ÿæˆçš„æ–‡ä»¶ä¸€è‡´
            data = yaml.safe_load(f)
            self.task_list = data.get('tasksh', data.get('tasks', []))

        self.init_realsense()
        self.detector = pipeline(model="IDEA-Research/grounding-dino-tiny", task="zero-shot-object-detection", device="cuda")
        
        if SAM_AVAILABLE:
            sam = sam_model_registry["vit_h"](checkpoint="/FoundationPose/weights/sam_vit_h_4b8939.pth").to("cuda")
            self.sam_predictor = SamPredictor(sam)

        self.pose_est = LegoPoseEstimator(ScorePredictor(), PoseRefinePredictor())

    def init_realsense(self):
        self.pipeline = rs.pipeline()
        config = rs.config()
        config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)
        config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)
        profile = self.pipeline.start(config)
        intr = profile.get_stream(rs.stream.color).as_video_stream_profile().get_intrinsics()
        self.K_MATRIX = np.array([[intr.fx, 0, intr.ppx], [0, intr.fy, intr.ppy], [0, 0, 1]])
        self.align = rs.align(rs.stream.color)

    def run(self):
        for task in self.task_list:
            name = task['name']
            print(f"\n" + "="*60 + f"\nğŸ¯ æ­£åœ¨è¯†åˆ«: {name}")
            
            mesh_path = self.get_mesh_path(name)
            self.pose_est.update_mesh(mesh_path)

            # --- é˜¶æ®µ 1: è§‚å¯Ÿ ---
            obs_start = time.time()
            best_det = None
            max_score = -1
            while (time.time() - obs_start) < 8.0:
                frames = self.pipeline.wait_for_frames()
                img = np.asanyarray(self.align.process(frames).get_color_frame().get_data())
                img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
                results = self.detector(img_pil, candidate_labels=[name, "lego block."], threshold=0.3)
                for r in results:
                    if r['score'] > max_score:
                        max_score = r['score']
                        best_det = DetectionResult(r['score'], r['label'], BoundingBox(**r['box']))
                        best_img_bgr = img.copy()
                cv2.imshow("Robot Assembly Vision", img); cv2.waitKey(1)

            if not best_det: continue

            # --- é˜¶æ®µ 2: åˆ†å‰² ---
            if SAM_AVAILABLE:
                self.sam_predictor.set_image(cv2.cvtColor(best_img_bgr, cv2.COLOR_BGR2RGB))
                masks, _, _ = self.sam_predictor.predict(box=np.array(best_det.box.xyxy), multimask_output=False)
                refined_mask = masks[0]
            else:
                refined_mask = np.zeros(best_img_bgr.shape[:2], dtype=bool)
                refined_mask[best_det.box.ymin:best_det.box.ymax, best_det.box.xmin:best_det.box.xmax] = True

            # --- é˜¶æ®µ 3: ç²¾ç‚¼ ---
            pose_samples = []
            refine_start = time.time()
            while (time.time() - refine_start) < 4.0:
                frames = self.pipeline.wait_for_frames()
                aligned = self.align.process(frames)
                img = np.asanyarray(aligned.get_color_frame().get_data())
                depth_m = np.asanyarray(aligned.get_depth_frame().get_data()).astype(np.float32) / 1000.0
                T_curr = self.pose_est.estimator.register(K=self.K_MATRIX, rgb=img, depth=depth_m, ob_mask=refined_mask, iteration=30)
                if T_curr is not None:
                    pose_samples.append(T_curr)
                    self.visualize_result(img, T_curr)
                cv2.imshow("Robot Assembly Vision", img); cv2.waitKey(1)

            if pose_samples:
                self.send_to_robot(name, pose_samples[-1], task)

    def visualize_result(self, image, T_cam_obj):
        length = 0.05
        axis_pts_3d = np.float32([[0,0,0], [length,0,0], [0,length,0], [0,0,length]])
        pts_cam = (T_cam_obj[:3, :3] @ axis_pts_3d.T).T + T_cam_obj[:3, 3]
        pts_2d = []
        for p in pts_cam:
            u = int(self.K_MATRIX[0,0] * p[0]/p[2] + self.K_MATRIX[0,2])
            v = int(self.K_MATRIX[1,1] * p[1]/p[2] + self.K_MATRIX[1,2])
            pts_2d.append((u, v))
        cv2.line(image, pts_2d[0], pts_2d[1], (0,0,255), 2)
        cv2.line(image, pts_2d[0], pts_2d[2], (0,255,0), 2)
        cv2.line(image, pts_2d[0], pts_2d[3], (255,0,0), 2)

    def send_to_robot(self, name, T_cam_obj, task_cfg):
        """ 
        æ’é™¤æ³•ç‰ˆæœ¬é€»è¾‘ï¼š
        1. Pick Pos: è§†è§‰å®æ—¶è§£ç®—çš„ä¸­å¿ƒç‚¹ (ä¿æŒä½ çš„è¦æ±‚ï¼šä¸æ”¹å˜)
        2. Pick Orientation: ç›´æ¥å¤åˆ¶ tasks.yaml é‡Œçš„å€¼ (ä¸å†å åŠ è§†è§‰æ£€æµ‹çš„è§’åº¦)
        3. Place Pos: åŸå§‹é€»è¾‘ (ASSEMBLY_CENTER_BASE + ä»»åŠ¡åç§»)
        4. Place Orientation: ç›´æ¥å¤åˆ¶ tasks.yaml é‡Œçš„å€¼
        """
        # A. æå–è§†è§‰ä¸­å¿ƒä½ç½® (Vision Position)
        T_base_vision = self.T_base_camera @ T_cam_obj
        vision_pick_pos = T_base_vision[:3, 3].tolist() 

        # B. ä»ä»»åŠ¡è¡¨ä¸­ç›´æ¥è¯»å–é¢„è®¾å§¿æ€ (Orientation)
        # è¿™åŒ…å«äº†ä½ åœ¨ plan.py é‡Œå®šå¥½çš„ [Roll=180, Pitch=0, Yaw=0/90]
        yaml_pick_orn = task_cfg['pick']['orientation']
        yaml_place_orn = task_cfg['place']['orientation']

        # C. è®¡ç®—æ”¾ç½®ä½ç½® (Place Position)
        # ä¿æŒä½ åŸå§‹çš„é€»è¾‘ï¼šåŸºå‡†ä¸­å¿ƒ + è§„åˆ’ä¸­çš„ç›¸å¯¹åç§»
        original_place_pos = (ASSEMBLY_CENTER_BASE + np.array(task_cfg['place']['pos'])).tolist()

        # D. å°è£…æ•°æ®å‘é€
        data = {
            'name': name,
            'pick': {
                'pos': vision_pick_pos,       # å¬è§†è§‰çš„
                'orientation': yaml_pick_orn  # å®Œå…¨å¬ tasks.yaml çš„
            },
            'place': {
                'pos': original_place_pos,    # å¬åŸå§‹é€»è¾‘çš„
                'orientation': yaml_place_orn # å®Œå…¨å¬ tasks.yaml çš„
            }
        }
        
        with open(RESULT_FILE, 'w') as f:
            yaml.dump(data, f)
            
        print(f"âš ï¸ æ¨¡å¼ï¼šå§¿æ€é”å®šã€‚Pick ä½¿ç”¨è§†è§‰ä¸­å¿ƒï¼Œä½†å§¿æ€å®Œå…¨éµå¾ª YAML è§„åˆ’ã€‚")
        print(f"   - å‘é€çš„ Pick åæ ‡: {vision_pick_pos}")
        
        # ç­‰å¾… C++ èŠ‚ç‚¹å¤„ç†
        while os.path.exists(RESULT_FILE):
            time.sleep(0.5)

    def get_mesh_path(self, task_name):
        keyword = "4x2" if "2x4" in task_name or "4x2" in task_name else "2x2"
        for f in os.listdir(MESH_DIR):
            if keyword in f and f.endswith(".stl"): return os.path.join(MESH_DIR, f)
        return ""

if __name__ == "__main__":
    node = RobotVisionNode()
    node.run()