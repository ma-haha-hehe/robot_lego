#!/usr/bin/env python3
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from PIL import Image as PILImage
from std_msgs.msg import String
from cv_bridge import CvBridge
import cv2
import numpy as np
import torch
import json
from dataclasses import dataclass
from typing import List, Optional, Dict, Union

# --- å¼•å…¥æ‚¨æä¾›çš„ yolo_sam.py ä¸­çš„æ ¸å¿ƒåº“ ---
from ultralytics import YOLOWorld
from transformers import AutoModelForMaskGeneration, AutoProcessor

# ==========================================
# 1. è¾…åŠ©ç±»å®šä¹‰ (æºè‡ªæ‚¨çš„ yolo_sam.py)
# ==========================================

@dataclass
class BoundingBox:
    xmin: int
    ymin: int
    xmax: int
    ymax: int

    @property
    def xyxy(self) -> List[float]:
        return [self.xmin, self.ymin, self.xmax, self.ymax]
    
    @property
    def center(self) -> List[int]:
        """è®¡ç®—ä¸­å¿ƒç‚¹åæ ‡"""
        return [int((self.xmin + self.xmax) / 2), int((self.ymin + self.ymax) / 2)]

@dataclass
class DetectionResult:
    score: float
    label: str
    box: BoundingBox
    mask: Optional[np.array] = None

# ==========================================
# 2. ROS 2 VLM èŠ‚ç‚¹ç±»
# ==========================================

class VLMNode(Node):
    def __init__(self):
        super().__init__('vlm_inference_node')
        
        # --- é…ç½®å‚æ•° ---
        # é»˜è®¤å¯»æ‰¾çš„ç›®æ ‡ (å¯ä»¥é€šè¿‡è¯é¢˜ä¿®æ”¹)
        self.target_labels = ["cube"]
        self.conf_threshold = 0.01  # é™ä½é˜ˆå€¼ä»¥æé«˜æ£€å‡ºç‡
        # --- å…³é”®ä¿®æ”¹ 2: é€‚é… Mac M3 (MPS) ---
        if torch.cuda.is_available():
            self.device = "cuda"
        elif torch.backends.mps.is_available():
            self.device = "mps"  # æ¿€æ´» Mac GPU åŠ é€Ÿ
        else:
            self.device = "cpu"
        
        self.get_logger().info(f"æ­£åœ¨åŠ è½½æ¨¡å‹åˆ° {self.device}ï¼Œè¯·ç¨å€™...")

        # --- åŠ è½½æ¨¡å‹ (YOLO-World + MobileSAM) ---
        try:
            # 1. åŠ è½½æ£€æµ‹å™¨ (YOLO-World) - è¿™é‡Œæ˜¯ä½ çš„"çœ¼ç›"
            self.detector = YOLOWorld("weights/yolov8s-world.pt") 
            self.detector.set_classes(self.target_labels)
            self.detector.to(self.device)
            
            # 2. åŠ è½½åˆ†å‰²å™¨ (MobileSAM) - å¯é€‰ï¼Œä¸ºäº†æ›´ç²¾ç»†
            self.segmenter_id = "facebook/sam-vit-base" # æˆ–è€…ä½¿ç”¨æ›´å¿«çš„ "mobile-sam"
            self.sam_model = AutoModelForMaskGeneration.from_pretrained(self.segmenter_id).to(self.device)
            self.sam_processor = AutoProcessor.from_pretrained(self.segmenter_id)
            
            self.get_logger().info("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")
        except Exception as e:
            self.get_logger().error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return

        # --- ROS é€šä¿¡æ¥å£ ---
        self.bridge = CvBridge()

        # 1. è®¢é˜…æ‘„åƒå¤´å›¾åƒ
        self.sub_image = self.create_subscription(
            Image,
            "/camera/color/image_raw",
            self.image_callback,
            1 # queue size 1ï¼Œä¿è¯åªå¤„ç†æœ€æ–°å¸§ï¼Œä¸ç§¯å‹
        )

        # 2. è®¢é˜… Prompt ä¿®æ”¹æŒ‡ä»¤ (ä¾‹å¦‚å‘é€ "cup" å°±ä¼šæ”¹ä¸ºæ‰¾æ¯å­)
        self.sub_prompt = self.create_subscription(
            String,
            "/vlm/prompt",
            self.prompt_callback,
            10
        )

        # 3. å‘å¸ƒæ£€æµ‹ç»“æœ (JSON æ ¼å¼çš„åæ ‡)
        self.pub_results = self.create_publisher(String, "/vlm/results", 10)

        # 4. å‘å¸ƒè°ƒè¯•å›¾åƒ (ç”»æ¡†çš„å›¾)
        self.pub_debug_img = self.create_publisher(Image, "/vlm/debug_image", 10)

        self.get_logger().info(f"VLM èŠ‚ç‚¹å·²å¯åŠ¨ã€‚é»˜è®¤å¯»æ‰¾: {self.target_labels}")

    def prompt_callback(self, msg):
        """åŠ¨æ€ä¿®æ”¹è¦å¯»æ‰¾çš„ç›®æ ‡"""
        new_labels = [label.strip() for label in msg.data.split(",")]
        self.target_labels = new_labels
        # YOLO-World ç‰¹æ€§ï¼šå¯ä»¥åœ¨è¿è¡Œæ—¶é‡æ–°è®¾ç½®ç±»åˆ«
        self.detector.set_classes(self.target_labels)
        self.get_logger().info(f"ğŸ”„ ç›®æ ‡åˆ—è¡¨å·²æ›´æ–°ä¸º: {self.target_labels}")

    def image_callback(self, msg):
        """æ ¸å¿ƒå¤„ç†å¾ªç¯"""
        try:
            # 1. è½¬æ¢å›¾åƒ ROS -> OpenCV
            cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")
            pil_image = cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB) # YOLO éœ€è¦ RGB
            pil_image = PILImage.fromarray(pil_image)
        except Exception as e:
            self.get_logger().error(f"å›¾åƒè½¬æ¢é”™è¯¯: {e}")
            return

        # 2. æ¨ç†: æ£€æµ‹ (YOLO-World)
        # verbose=False é˜²æ­¢åˆ·å±
        results = self.detector.predict(pil_image, conf=self.conf_threshold, verbose=False)
        
        detections = []
        if len(results) > 0:
            result = results[0]
            boxes = result.boxes.xyxy.cpu().tolist()
            scores = result.boxes.conf.cpu().tolist()
            cls_ids = result.boxes.cls.cpu().tolist()
            names = result.names

            for box, score, cls_id in zip(boxes, scores, cls_ids):
                label = names[int(cls_id)]
                det = DetectionResult(
                    score=score,
                    label=label,
                    box=BoundingBox(
                        xmin=int(box[0]), ymin=int(box[1]),
                        xmax=int(box[2]), ymax=int(box[3])
                    )
                )
                detections.append(det)

        # 3. (å¯é€‰) æ¨ç†: åˆ†å‰² (SAM)
        # å¦‚æœåªéœ€è¦æŠ“å–åæ ‡ï¼Œå…¶å® Box Center å°±å¤Ÿäº†ï¼ŒSAM ä¼šå¢åŠ è®¡ç®—è€—æ—¶ã€‚
        # è¿™é‡Œä¸ºäº†ä¿æŒå’Œä½ æä¾›çš„åŠŸèƒ½ä¸€è‡´ï¼Œæˆ‘åŠ ä¸Šäº†ã€‚
        if len(detections) > 0:
            detections = self.run_sam(pil_image, detections)

        # 4. æ‰“åŒ…ç»“æœå¹¶å‘å¸ƒ
        results_json = []
        
        # ç”¨äºç”»å›¾çš„ç”»å¸ƒ
        debug_img = cv_image.copy()

        for det in detections:
            # --- æ•°æ®æ‰“åŒ… ---
            center = det.box.center
            obj_data = {
                "label": det.label,
                "score": round(det.score, 2),
                "center_x": center[0],
                "center_y": center[1],
                "bbox": det.box.xyxy
            }
            results_json.append(obj_data)

            # --- ç”»å›¾ (Debug) ---
            # ç”»æ¡†
            cv2.rectangle(debug_img, (det.box.xmin, det.box.ymin), 
                          (det.box.xmax, det.box.ymax), (0, 255, 0), 2)
            # ç”»ä¸­å¿ƒç‚¹
            cv2.circle(debug_img, (center[0], center[1]), 5, (0, 0, 255), -1)
            # å†™å­—
            cv2.putText(debug_img, f"{det.label}: {center}", 
                        (det.box.xmin, det.box.ymin - 10), 
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

        # å‘å¸ƒ JSON å­—ç¬¦ä¸²
        if results_json:
            msg_str = String()
            msg_str.data = json.dumps(results_json)
            self.pub_results.publish(msg_str)
            self.get_logger().info(f"æ£€æµ‹åˆ°: {msg_str.data}")

        # å‘å¸ƒ Debug å›¾åƒ
        try:
            debug_msg = self.bridge.cv2_to_imgmsg(debug_img, "bgr8")
            self.pub_debug_img.publish(debug_msg)
        except Exception as e:
            pass

    def run_sam(self, image_pil, detections):
        """è¿è¡Œ SAM è¿›è¡Œåˆ†å‰² (ç›´æ¥å¤ç”¨ä½ çš„é€»è¾‘)"""
        # æå– boxes
        input_boxes = [[d.box.xyxy for d in detections]] # SAM éœ€è¦è¿™ç§åµŒå¥— list
        
        if not input_boxes or len(input_boxes[0]) == 0:
            return detections

        inputs = self.sam_processor(images=image_pil, input_boxes=input_boxes, return_tensors="pt")
        inputs = {k: v.to(self.device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = self.sam_model(**inputs)

        masks = self.sam_processor.post_process_masks(
            masks=outputs.pred_masks,
            original_sizes=inputs["original_sizes"],
            reshaped_input_sizes=inputs["reshaped_input_sizes"]
        )[0]
        
        # å°† mask å­˜å› detection å¯¹è±¡ (è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œåªåšæ¨ç†ä¸ä¸€å®šè¦å­˜å›)
        # å› ä¸ºæˆ‘ä»¬ä¸»è¦ç›®çš„æ˜¯è¾“å‡ºåæ ‡ï¼ŒMask ä¸»è¦ç”¨äºæ›´é«˜çº§çš„é¿éšœæˆ–ç²¾ç»†æŠ“å–
        return detections

# ==========================================
# 3. ä¸»ç¨‹åºå…¥å£
# ==========================================
def main(args=None):
    from PIL import Image # ç¡®ä¿å†…éƒ¨èƒ½å¼•ç”¨
    rclpy.init(args=args)
    
    node = VLMNode()
    
    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    finally:
        node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()